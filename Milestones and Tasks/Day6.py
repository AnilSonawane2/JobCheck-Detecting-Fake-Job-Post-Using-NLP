# -*- coding: utf-8 -*-
"""Sample.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RoZ1f_sTgzczZ4K70jUvOVlykBH_fVq5

# Task 1
"""

import pandas as pd
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download resources
nltk.download('stopwords')
nltk.download('wordnet')

# Load dataset
df = pd.read_csv('fake_job_postings.csv')

# Define text cleaning function
def clean_text(text):
    if pd.isnull(text):
        return ""
    # 1. Lowercase
    text = text.lower()
    # 2. Remove HTML tags
    text = re.sub(r'<.*?>', ' ', text)
    # 3. Remove URLs
    text = re.sub(r'http\S+|www\S+', '', text)
    # 4. Remove punctuation and numbers
    text = re.sub(r'[%s\d]' % re.escape(string.punctuation), ' ', text)
    # 5. Remove extra spaces
    text = re.sub(r'\s+', ' ', text).strip()
    # 6. Remove stopwords and lemmatize
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]
    return " ".join(words)

# Apply cleaning to key text columns
df['clean_description'] = df['description'].apply(clean_text)

# Show before and after
print("Original Text:\n", df['description'].iloc[1][:300])
print("\nCleaned Text:\n", df['clean_description'].iloc[1][:300])

# Check for any remaining issues
print("\nExample of Cleaned Data:")
print(df[['description', 'clean_description']].head(3))

"""# Task 2"""

# Count number of words before cleaning
df['desc_word_count_before'] = df['description'].fillna("").apply(lambda x: len(x.split()))

# Count number of words after cleaning
df['desc_word_count_after'] = df['clean_description'].fillna("").apply(lambda x: len(x.split()))

# Calculate averages
avg_before = df['desc_word_count_before'].mean()
avg_after = df['desc_word_count_after'].mean()

print(f"Average words before cleaning: {avg_before:.2f}")
print(f"Average words after cleaning:  {avg_after:.2f}")

# If the average word count dropped moderately (e.g., from 120 → 80 words)
# If the word count dropped too much (e.g., from 120 → 20 words)

"""# Day 4 : Code"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import pandas as pd
df = pd.read_csv('fake_job_postings.csv')

texts = df['clean_description'].fillna('').tolist()

# Bag-of-Words
bow_vectorizer = CountVectorizer(max_features=2000)  # limit to top 2000 words
X_bow = bow_vectorizer.fit_transform(texts)
# BoW counts how many times each word appears in a text.
print("BoW shape:", X_bow.shape)
print("Sample feature names (BoW):", bow_vectorizer.get_feature_names_out()[:10])

# TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=2000)
X_tfidf = tfidf_vectorizer.fit_transform(texts)

print("\nTF-IDF shape:", X_tfidf.shape) # Similar to BoW but gives less weight to common words
print("Sample feature names (TF-IDF):", tfidf_vectorizer.get_feature_names_out()[:10])

# Compare sparsity and values
print("\nExample BoW vector (first row):")
print(X_bow[0].toarray())

print("\nExample TF-IDF vector (first row):")
print(X_tfidf[0].toarray())

"""# Task 1"""

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

df = pd.read_csv('fake_job_postings.csv')

texts = df['company_profile'].fillna('').tolist()

bow_vectorizer = CountVectorizer(max_features=2000)
X_bow = bow_vectorizer.fit_transform(texts)

tfidf_vectorizer = TfidfVectorizer(max_features=2000)
X_tfidf = tfidf_vectorizer.fit_transform(texts)

# Compare shapes
print("BoW shape:", X_bow.shape)
#
print("TF-IDF shape:", X_tfidf.shape)

"""# Task 2"""

# Get job descriptions
desc_texts = df['description'].fillna('').tolist()

# Create BoW model
desc_vectorizer = CountVectorizer(max_features=5000)
X_desc_bow = desc_vectorizer.fit_transform(desc_texts)

# Get word frequencies (sum across all documents)
word_freq = X_desc_bow.sum(axis=0).A1  # convert sparse matrix to array
words = desc_vectorizer.get_feature_names_out()

# Create a DataFrame of words and their counts
freq_df = pd.DataFrame({'word': words, 'count': word_freq})

# Sort and display top 20 words
top20 = freq_df.sort_values(by='count', ascending=False).head(20)
print("\nTop 20 most frequent words in job descriptions (using BoW):")
print(top20)