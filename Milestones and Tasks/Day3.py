# -*- coding: utf-8 -*-
"""Oct23-Tasks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nkodPPTLGMOjHT-YS_hsl0sMI3_Cc6U7
"""

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

df = pd.read_csv('fake_job_postings.csv')

"""# Task 1"""

texts = df['company_profile'].fillna('').tolist()

bow_vectorizer = CountVectorizer(max_features=2000)
X_bow = bow_vectorizer.fit_transform(texts)

tfidf_vectorizer = TfidfVectorizer(max_features=2000)
X_tfidf = tfidf_vectorizer.fit_transform(texts)

# Compare shapes
print("BoW shape:", X_bow.shape)
print("TF-IDF shape:", X_tfidf.shape)

# Discussion
print("""
- BoW only counts how often words appear. It treats all words equally,
  so common but uninformative words ('company', 'team') dominate.
- TF-IDF reduces the weight of frequent words and increases the importance
  of unique or discriminative words. It captures meaning and importance better.

=> Therefore, TF-IDF captures the semantics of the text more effectively.""")

"""# Task 2"""

# Get job descriptions
desc_texts = df['description'].fillna('').tolist()

# Create BoW model
desc_vectorizer = CountVectorizer(max_features=5000)
X_desc_bow = desc_vectorizer.fit_transform(desc_texts)

# Get word frequencies (sum across all documents)
word_freq = X_desc_bow.sum(axis=0).A1  # convert sparse matrix to array
words = desc_vectorizer.get_feature_names_out()

# Create a DataFrame of words and their counts
freq_df = pd.DataFrame({'word': words, 'count': word_freq})

# Sort and display top 20 words
top20 = freq_df.sort_values(by='count', ascending=False).head(20)
print("\nTop 20 most frequent words in job descriptions (using BoW):")
print(top20)