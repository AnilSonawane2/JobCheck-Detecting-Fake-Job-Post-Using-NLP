# -*- coding: utf-8 -*-
"""Day-7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19NmcNld3wrQOmYcAZFFkgU4t4GgjN158

# Day 7: Model Evaluation & Hyperparameter Tuning
"""

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, roc_curve, classification_report
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re, html

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt_tab')

# Load cleaned data
df = pd.read_csv('fake_job_postings.csv')
stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    """
    Clean raw job posting text:
    - Unescape HTML
    - Lowercase
    - Remove URLs, emails, numbers, punctuation
    - Tokenize
    - Remove stopwords
    - Lemmatize using POS tagging
    """
    if pd.isna(text):
        return ""

    text = html.unescape(str(text))
    text = text.lower()
    text = re.sub(r'http\S+|www\S+', '', text)
    text = re.sub(r'\S+@\S+', '', text)
    text = re.sub(r'[^a-z\s]', '', text)

    tokens = nltk.word_tokenize(text)
    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]

    lemmatized = [lemmatizer.lemmatize(t) for t in tokens]
    return " ".join(lemmatized)

df["clean_description"] = df["description"].apply(clean_text)
df = df.dropna(subset=['clean_description'])

# TF-IDF vectorization
vectorizer = TfidfVectorizer(max_features=3000)
X = vectorizer.fit_transform(df['clean_description'])
y = df['fraudulent']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Initialize models
log_reg = LogisticRegression(max_iter=200)
rf = RandomForestClassifier(random_state=42)

# 1️⃣ Cross-validation (5-fold)

log_cv = cross_val_score(log_reg, X_train, y_train, cv=5, scoring='accuracy')
rf_cv = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy')
print("Logistic Regression CV Accuracy:", log_cv.mean())
print("Random Forest CV Accuracy:", rf_cv.mean())

# 2️⃣ Fit models

log_reg.fit(X_train, y_train)
rf.fit(X_train, y_train)

# 3️⃣ ROC-AUC Comparison

y_prob_log = log_reg.predict_proba(X_test)[:, 1]
y_prob_rf = rf.predict_proba(X_test)[:, 1]
fpr1, tpr1, _ = roc_curve(y_test, y_prob_log)
fpr2, tpr2, _ = roc_curve(y_test, y_prob_rf)

plt.figure(figsize=(8,5))
plt.plot(fpr1, tpr1, label="Logistic Regression")
plt.plot(fpr2, tpr2, label="Random Forest")
plt.plot([0,1], [0,1], 'k--', label='Random Chance')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend()
plt.show()
print("Logistic Regression AUC:", roc_auc_score(y_test, y_prob_log))
print("Random Forest AUC:", roc_auc_score(y_test, y_prob_rf))

# 4️⃣ Hyperparameter tuning (GridSearchCV on Random Forest)
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5],
}

grid = GridSearchCV(rf, param_grid, cv=3, scoring='accuracy', n_jobs=-1)
grid.fit(X_train, y_train)
print("Best Parameters:", grid.best_params_)
print("Best Cross-Validation Accuracy:", grid.best_score_)

"""Task 1 - Cross-Validation Analysis"""

log_reg = LogisticRegression(max_iter=200, random_state=42)
dt = DecisionTreeClassifier(random_state=42)
rf = RandomForestClassifier(random_state=42)

models = {
    "Logistic Regression": log_reg,
    "Decision Tree": dt,
    "Random Forest": rf
}

cv_means = {}
cv_stds = {}

print("=== 5-Fold Cross-Validation Results ===\n")
for name, model in models.items():
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    cv_means[name] = cv_scores.mean()
    cv_stds[name] = cv_scores.std()
    print(f"{name}: Mean Accuracy = {cv_scores.mean():.4f}, Std Dev = {cv_scores.std():.4f}")

# Bar chart of mean accuracies

plt.figure(figsize=(8,5))
plt.bar(cv_means.keys(), cv_means.values(), color=['skyblue', 'lightgreen', 'salmon'])
plt.title("Model Comparison: Mean CV Accuracy (5-Fold)")
plt.ylabel("Mean Accuracy")
plt.ylim(0, 1)
plt.show()

# Most stable model (lowest variance)
stable_model = min(cv_stds, key=cv_stds.get)
print(f"\nMost stable model (lowest variance): {stable_model}\n")

"""Task 2 - ROC-AUC Visualization"""

# Fit models
for model in models.values():
    model.fit(X_train, y_train)

# Predict probabilities
y_prob_log = log_reg.predict_proba(X_test)[:, 1]
y_prob_dt = dt.predict_proba(X_test)[:, 1]
y_prob_rf = rf.predict_proba(X_test)[:, 1]

# Compute ROC curve

fpr1, tpr1, _ = roc_curve(y_test, y_prob_log)
fpr2, tpr2, _ = roc_curve(y_test, y_prob_dt)
fpr3, tpr3, _ = roc_curve(y_test, y_prob_rf)

# Plot ROC curves
plt.figure(figsize=(8,6))
plt.plot(fpr1, tpr1, label=f"Logistic Regression (AUC={roc_auc_score(y_test, y_prob_log):.3f})")
plt.plot(fpr2, tpr2, label=f"Decision Tree (AUC={roc_auc_score(y_test, y_prob_dt):.3f})")
plt.plot(fpr3, tpr3, label=f"Random Forest (AUC={roc_auc_score(y_test, y_prob_rf):.3f})")
plt.plot([0,1], [0,1], 'k--', label='Random Chance')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison")
plt.legend()
plt.show()

print("\n=== AUC Scores ===")
print(f"Logistic Regression AUC: {roc_auc_score(y_test, y_prob_log):.4f}")
print(f"Decision Tree AUC:       {roc_auc_score(y_test, y_prob_dt):.4f}")
print(f"Random Forest AUC:       {roc_auc_score(y_test, y_prob_rf):.4f}")

best_auc_model = max(
    {
        "Logistic Regression": roc_auc_score(y_test, y_prob_log),
        "Decision Tree": roc_auc_score(y_test, y_prob_dt),
        "Random Forest": roc_auc_score(y_test, y_prob_rf)
    },
    key=lambda k: {
        "Logistic Regression": roc_auc_score(y_test, y_prob_log),
        "Decision Tree": roc_auc_score(y_test, y_prob_dt),
        "Random Forest": roc_auc_score(y_test, y_prob_rf)
    }[k]
)

# Compute AUCs once
auc_scores = {
    "Logistic Regression": roc_auc_score(y_test, y_prob_log),
    "Decision Tree": roc_auc_score(y_test, y_prob_dt),
    "Random Forest": roc_auc_score(y_test, y_prob_rf)
}

# Find best model
best_auc_model = max(auc_scores, key=auc_scores.get)

print(f"\nModel with best AUC: {best_auc_model} ({auc_scores[best_auc_model]:.4f})\n")

"""Task 3 – Hyperparameter Tuning Challenge (Optional)"""

param_grid = {
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5],
    'criterion': ['gini', 'entropy']
}

grid_dt = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=3,
    scoring='accuracy',
    n_jobs=-1
)

grid_dt.fit(X_train, y_train)

print("=== Grid Search Results (Decision Tree) ===")
print("Best Parameters:", grid_dt.best_params_)
print(f"Best Cross-Validation Accuracy: {grid_dt.best_score_:.4f}")

# Evaluate tuned Decision Tree vs Random Forest
best_dt = grid_dt.best_estimator_
best_dt.fit(X_train, y_train)
y_pred_dt = best_dt.predict(X_test)
y_pred_rf = rf.predict(X_test)

print("\nTuned Decision Tree Test Accuracy:", best_dt.score(X_test, y_test))
print("Random Forest Test Accuracy:", rf.score(X_test, y_test))

print("""
Interpretation:
Hyperparameter tuning improved the Decision Tree by controlling depth and split criteria.
This reduces overfitting and makes the model more generalizable.
However, Random Forest often still performs slightly better due to ensemble averaging.
""")