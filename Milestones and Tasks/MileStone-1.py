# -*- coding: utf-8 -*-
"""Milestone1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BZMoDLIJCphcdDIInNZ8qFZOlognnwDf
"""

import re
import html
import string
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt')
nltk.download('punkt_tab')

"""# Data Collection

Step 1 : Load Dataset
"""

df = pd.read_csv('fake_job_postings.csv')

print("Shape of dataset:", df.shape)
print("Columns:", df.columns.tolist())

"""# Data Preprocessing

Step 2 : Handle the Missing Values
"""

text_columns = ['title', 'company_profile', 'description', 'requirements', 'benefits']
for col in text_columns:
    df[col] = df[col].fillna('')

"""Step 3 : Combine text column"""

# Combine multiple text columns into one large column for NLP
df['combined_text'] = (
    df['title'] + ' ' +
    df['company_profile'] + ' ' +
    df['description'] + ' ' +
    df['requirements'] + ' ' +
    df['benefits']
)

"""Step 4 : Text Cleaning Function"""

def clean_text(text):           # Data Cleaning Function
    """
    Clean raw job posting text:
    - Unescape HTML
    - Lowercase
    - Remove URLs, numbers, punctuation
    - Remove stopwords
    - Lemmatize words
    """
    # Unescape HTML entities (e.g., &amp; -> &)
    text = html.unescape(str(text))

    # Convert to lowercase
    text = text.lower()

    # Remove URLs and emails
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)

    # Remove digits and punctuation
    text = re.sub(r'\d+', '', text)
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Tokenize
    tokens = nltk.word_tokenize(text)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]

    # Lemmatize
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    # Join tokens back to string
    cleaned_text = ' '.join(tokens)

    return cleaned_text

"""Step 5 : Applying Cleaning"""

df['clean_text'] = df['combined_text'].apply(clean_text)

"""# Feature Extraction

Step 6 : Extract Features using TF-IDF
"""

tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
X = tfidf.fit_transform(df['clean_text'])

"""Step 7 : Prepare Target Variable"""

y = df['fraudulent']

"""Showing the Output"""

print("TF-IDF Feature Matrix Shape:", X.shape)
print("\nExample feature names:", tfidf.get_feature_names_out()[:10])
print("\nTarget variable distribution:")
print(y.value_counts())