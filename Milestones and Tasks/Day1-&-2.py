# -*- coding: utf-8 -*-
"""Day1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12NHzZif4qQlRWx6lLYRUSkeW558Jd7WX
"""

import pandas as pd

df = pd.read_csv('fake_job_postings.csv')
# Display first few rows
#print("Sample Data:")
print(df.head())

# Display basic info
print('-------------------------------')
print("Dataset Info:")
print(df.info())
# Check for missing values
print('-------------------------------')
print("Missing Values per Column:")
print(df.isnull().sum())

# Check distribution of target variable
print('-------------------------------')
print("Target (fraudulent) Distribution:")
print(df['fraudulent'].value_counts())
# how many times each unique value appears

# Basic statistics
print('-------------------------------')
print("\nDataset Summary:")
print(df.describe(include='all'))

import pandas as pd

# Load the dataset
df = pd.read_csv('fake_job_postings.csv')

# Total number of records
print("Total number of records:", len(df))
print('\n')
print('------------------------------------------------------')

# Number of missing values per column
print("Missing Values per Column: \n")
print(df.isnull().sum())
print('\n')
print('------------------------------------------------------')

# Count of real vs fake jobs
print("Count of Real vs Fake Jobs: \n")
print(df['fraudulent'].value_counts())
print('\n')
print('------------------------------------------------------')

# Display 3 examples of fake job descriptions
print("Examples of Fake Job Descriptions : \n")
fake_jobs = df[df['fraudulent'] == 1]   # filter fake jobs
print(fake_jobs['description'].head(3)) # show 3 examples
print('\n')
print('------------------------------------------------------')

# column with the most missing values
missing_counts = df.isnull().sum()
most_missing_feature = missing_counts.idxmax()
most_missing_count = missing_counts.max()

print(f'Column with most missing values is : {most_missing_feature} : {most_missing_count}')

# Day 3

import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger_eng')

text = "The students are studying NLP in the labs"
# Tokenize
tokens = word_tokenize(text)

# Apply POS
pos_tags = nltk.pos_tag(tokens)

print(pos_tags)

import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger_eng')

text = "Artificial Intelligence is transforming the world rapidly."
tokens = word_tokenize(text)

pos_tags = nltk.pos_tag(tokens)

for word, tag in pos_tags:
    print(f"{word} :- {tag}")

# Day 2

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

nltk.download('punkt_tab')
nltk.download('wordnet')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()

text = "The studies were running better than expected"
tokens = word_tokenize(text)

lem_words = [lemmatizer.lemmatize(word) for word in tokens]

print("Original Words : ",tokens)
print("After Lemmatization: ",lem_words)

from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

text = "Artificial Intelligence is transforming the world"
tokens = word_tokenize(text)
print(tokens)
ps = PorterStemmer()
stemmed_words = [ps.stem(word) for word in tokens]
print(stemmed_words)

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt_tab')

stemmer = PorterStemmer()

text = "He was studies and beautifully explained it easily"
tokens = word_tokenize(text)

stemmed_words = [stemmer.stem(word) for word in tokens]

print("Original word : ",tokens)
print("After Stemming : ",stemmed_words)

import nltk
nltk.download('punkt_tab')
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

text = "Players are running and studying regularly."
tokens = word_tokenize(text)

stemmed_words = [stemmer.stem(word) for word in tokens]
lem_words = [lemmatizer.lemmatize(word) for word in tokens]

print("Original text : ",text)
print("Stemmed text : ",stemmed_words)
print("Lemmatized text : ",lem_words)

# Day 1

import nltk
nltk.download('punkt')
nltk.download('punkt_tab') # Important
from nltk.tokenize import word_tokenize

text1 = "Data Science is Fun!"
text2 = "Hello, My Name is Anil"

# tokenize text into words
token1 = word_tokenize(text1)
token2 = word_tokenize(text2)

print("Token 1 : ", token1)
print("Token 2 : ", token2)